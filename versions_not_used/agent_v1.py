import boto3
import json

session = boto3.Session(profile_name="default", region_name="us-west-2")

# --------------------------------------------------------------------------------------------


"""
# Appel Ã  Bedrock Runtime
bedrock_runtime = session.client("bedrock-runtime")

prompt = {
    "inputText": "What's something cool about AI?"
}

payload = json.dumps(prompt)

response = bedrock_runtime.invoke_model(
    modelId="amazon.titan-text-express-v1",
    body=payload.encode("utf-8"),
    contentType="application/json",
    accept="application/json"
)

# RÃ©cupÃ©ration et parsing de la rÃ©ponse JSON
result_raw = response["body"].read()
result_json = json.loads(result_raw)

# ğŸŒŸ Affichage structurÃ© de toutes les infos
print("\n=== ğŸ“Š RÃ©sultat complet de Titan ===\n")

print(f"ğŸ”¢ Tokens en entrÃ©e : {result_json.get('inputTextTokenCount', 'N/A')}")
print(f"ğŸ”¢ Tokens en sortie : {result_json['results'][0].get('tokenCount', 'N/A')}")
print(f"âœ… Raison de fin : {result_json['results'][0].get('completionReason', 'N/A')}")
print("\nğŸ§  RÃ©ponse du modÃ¨le :\n")
print(result_json['results'][0].get('outputText', '').strip())
"""

# --------------------------------------------------------------------------------------------

# Ã‰tape 1 : Setup

import boto3
import pandas as pd
import time
from langchain_community.document_loaders import PyPDFLoader
from langchain.text_splitter import RecursiveCharacterTextSplitter
from langchain.chains import RetrievalQA
from langchain_community.vectorstores import FAISS
from langchain_aws import BedrockEmbeddings
from langchain_aws import BedrockLLM
from langchain.docstore.document import Document

# Client AWS
bedrock_runtime = boto3.client("bedrock-runtime", region_name="us-west-2")
redshift_client = boto3.client("redshift-data", region_name="us-west-2")

# Redshift config
DATABASE = "dev"
WORKGROUP = "redshift-wg-hackathon"





# Ã‰tape 2 : RÃ©cupÃ©ration des donnÃ©es Redshift

def get_redshift_data():
    query = "SELECT * FROM clients_database;"
    response = redshift_client.execute_statement(Database=DATABASE, WorkgroupName=WORKGROUP, Sql=query)
    statement_id = response["Id"]

    while True:
        status = redshift_client.describe_statement(Id=statement_id)
        if status["Status"] in ["FINISHED", "FAILED", "ABORTED"]:
            break
        time.sleep(1)

    results = redshift_client.get_statement_result(Id=statement_id)
    columns = [col["name"] for col in results["ColumnMetadata"]]
    rows = [
        [field.get("stringValue", "") for field in record]
        for record in results["Records"]
    ]
    return pd.DataFrame(rows, columns=columns)

df_clients = get_redshift_data()
print("DataFrame clients :")
print(f"Nombre de lignes : {len(df_clients)}")
print(df_clients)







# Ã‰tape 3 : Charger les deux PDFs et la base de donnÃ©es dans un systÃ¨me RAG
print("\n\n\n\n")

import os
from langchain.docstore.document import Document
from langchain_aws import BedrockEmbeddings  # version Ã  jour âœ…

# Charger les 2 fichiers PDF
loader1 = PyPDFLoader("Explanations_about_each_columns_of_the_clients_dataset.pdf")
loader2 = PyPDFLoader("The_different_types_of_insurance_contracts_and_details.pdf")

docs = loader1.load() + loader2.load()
print(f"âœ… PDF chargÃ©s : {len(docs)} pages")

# ğŸ” SÃ©lectionner les 1000 premiers clients uniquement
df_clients_sample = df_clients.head(1000)

# ğŸ” Ajouter les clients dans le corpus RAG
def clients_to_documents(df):
    docs = []
    for _, row in df.iterrows():
        content = "\n".join([f"{col} : {row[col]}" for col in df.columns])
        doc = Document(page_content=content, metadata={"source": "clients_database"})
        docs.append(doc)
    return docs

client_docs = clients_to_documents(df_clients_sample)
docs += client_docs

# âœ… Chemin de l'index sauvegardÃ©
faiss_path = "faiss_index/"

# Embeddings avec classe officielle (langchain_aws)
embeddings = BedrockEmbeddings(
    model_id="amazon.titan-embed-text-v1",
    region_name="us-west-2"
)

# âš¡ï¸ Sauvegarde / chargement automatique de FAISS
if os.path.exists(faiss_path):
    print("ğŸ“¦ Index FAISS dÃ©jÃ  existant, chargement en cours...")
    vectorstore = FAISS.load_local(faiss_path, embeddings)
else:
    print("ğŸ§  CrÃ©ation du FAISS Ã  partir des documents...")
    text_splitter = RecursiveCharacterTextSplitter(chunk_size=1000, chunk_overlap=100)
    chunks = text_splitter.split_documents(docs)
    print(f"ğŸ§© Total chunks gÃ©nÃ©rÃ©s : {len(chunks)}")
    
    vectorstore = FAISS.from_documents(chunks, embedding=embeddings)
    vectorstore.save_local(faiss_path)
    print(f"âœ… Index FAISS sauvegardÃ© Ã  lâ€™emplacement : {faiss_path}")

retriever = vectorstore.as_retriever()
print("âœ… RAG prÃªt avec FAISS + Titan + Clients")









# Ã‰tape 4 : Lancer lâ€™agent amazon.titan-text-express-v1 en mode RAG
print("\n\n\n\n")

# ğŸ” TEST : recherche de clients Ã  risque avec revenu faible
from langchain.chains import RetrievalQA
from langchain_aws import BedrockLLM  # version Ã  jour

# Initialise Titan Text Express
llm = BedrockLLM(
    model_id="amazon.titan-text-express-v1",
    region_name="us-west-2",
    model_kwargs={
        "temperature": 0.3,
        "maxTokenCount": 2048,
        "topP": 0.9
    }
)

# CrÃ©er la chaÃ®ne RAG
qa_chain = RetrievalQA.from_chain_type(
    llm=llm,
    retriever=retriever,
    return_source_documents=False
)

# ğŸ” Prompt de test
prompt_test = """
Quels clients parmi la base sont Ã  risque Ã©levÃ© et pourraient avoir besoin d'un ajustement de contrat ?
RÃ©ponds uniquement s'il y a un besoin de changement, avec leur Client_ID, et les raisons.
"""

response = qa_chain.invoke(prompt_test)

print("\nğŸ” RÃ‰PONSE DE L'AGENT A1 (TEST)")
print(response)





















